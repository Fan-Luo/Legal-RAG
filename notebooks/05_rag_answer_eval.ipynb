{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Answer Evaluation（合同法端到端评估）\n",
        "\n",
        "本 Notebook 用来对 **Legal-RAG** 在《民法典·合同编》场景下做 **端到端 RAG 答案评估**：\n",
        "\n",
        "- 检索：Hybrid / Graph-aware 等在 `RagPipeline` 中已经封装好\n",
        "- 生成：使用配置好的 LLM（Qwen / OpenAI 等）生成法律问答\n",        
        "- 评估：\n",
        "  - 检查答案是否覆盖关键法律要点（keyword-based heuristic）\n",
        "  - 检查是否引用了目标条文 / 相关条文\n",
        "  - 给出评分 + 汇总统计\n",
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 环境准备与配置\n",
        "\n",
        "- 需要先在根目录下完成：\n",
        "  - `python -m scripts.preprocess_law`\n",
        "  - `python -m scripts.build_index`\n",
        "- `data/processed` 和 `data/index` 已就绪\n",
        "默认使用 `RagPipeline` 封装好的：\n",
        "\n",
        "- 路由（QueryRouter）\n",
        "- Hybrid + Graph-aware 检索\n",
        "- LLM 问答\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from legalrag.config import AppConfig\n",
        "from legalrag.models import RagAnswer\n",
        "from legalrag.pipeline.rag_pipeline import RagPipeline\n",
        "from legalrag.utils.logger import get_logger\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "logger = get_logger(__name__)\n",
        "\n",
        "# 加载配置\n",
        "cfg = AppConfig.load()\n",
        "BASE_DIR = Path(cfg.paths.base_dir)\n",
        "DATA_DIR = Path(cfg.paths.data_dir)\n",
        "EVAL_DIR = Path(cfg.paths.eval_dir)\n",
        "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"BASE_DIR:\", BASE_DIR)\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"EVAL_DIR:\", EVAL_DIR)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 选择 / 修改 LLM 配置（可选）\n",
        "\n",
        "可以在 Notebook 中修改 LLM 类型，例如：\n",
        "\n",
        "- 使用本地 Qwen：`qwen-local + Qwen/Qwen2-1.5B-Instruct`\n",
        "- 使用 OpenAI：`openai + gpt-4.1-mini`（需环境中有 `OPENAI_API_KEY`）\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 可根据需要更改LLM 配置\n",
        "USE_QWEN_LOCAL = True\n",
        "USE_OPENAI = False  \n",
        "\n",
        "if hasattr(cfg, \"llm\"):\n",
        "    if USE_QWEN_LOCAL:\n",
        "        cfg.llm.provider = \"qwen-local\"\n",
        "        cfg.llm.model = \"Qwen/Qwen2-1.5B-Instruct\"\n",
        "    elif USE_OPENAI:\n",
        "        cfg.llm.provider = \"openai\"\n",
        "        cfg.llm.model = \"gpt-4.1-mini\"\n",
        "\n",
        "print(\"LLM provider:\", cfg.llm.provider)\n",
        "print(\"LLM model:\", cfg.llm.model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 初始化 RagPipeline\n",
        "\n",
        "- 加载检索组件（Hybrid / Graph / Router 等）\n",
        "- 加载 LLM（可能耗时，尤其是本地 Qwen），也可以换成 OpenAI 小模型（注意 API 费用）\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pipeline = RagPipeline(cfg)\n",
        "print(\"RagPipeline initialized with provider:\", cfg.llm.provider, \"model:\", cfg.llm.model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 构造 / 加载带“参考答案要点”的评估数据集\n",
        "\n",
        "在 `data/eval/contract_law_qa_answers.jsonl` 中使用如下结构：\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"question\": \"合同约定的违约金为合同金额的 40%，是否合理？\",\n",
        "  \"target_articles\": [\"违约金\", \"过高\", \"调整\"],\n",
        "  \"key_points\": [\n",
        "    \"可以请求人民法院或者仲裁机构适当减少违约金\",\n",
        "    \"显著过高的违约金可以调整\",\n",
        "    \"违约金条款应兼顾实际损失和合同履行情况\"\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "- `question`：评估问题\n",
        "- `target_articles`：主要目标条文 / 关键词列表 \n",
        "- `key_points`：希望答案中明确提到的关键法律要点\n",
        "\n",
        "如果文件不存在，会自动写入一个小 toy 集合。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "answers_eval_path = EVAL_DIR / \"contract_law_qa_answers.jsonl\"\n",
        "\n",
        "if not answers_eval_path.exists():\n",
        "    print(\"[INFO] answers eval file not found, creating a small toy set at:\", answers_eval_path)\n",
        "    toy_data = [\n",
        "        {\n",
        "            \"question\": \"合同约定的违约金为合同金额的 40%，是否合理？\",\n",
        "            \"target_articles\": [\"违约金\", \"过高\", \"调整\"],\n",
        "            \"key_points\": [\n",
        "                \"显著过高的违约金可以请求人民法院或者仲裁机构予以适当减少\",\n",
        "                \"判断违约金是否过高要结合实际损失和当事人过错等因素\",\n",
        "                \"不能简单以固定比例认定合理性\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"什么是不可抗力？\",\n",
        "            \"target_articles\": [\"不可抗力\"],\n",
        "            \"key_points\": [\n",
        "                \"不可抗力是不能预见、不能避免并不能克服的客观情况\",\n",
        "                \"发生不可抗力的一方可以部分或者全部免除责任\",\n",
        "                \"但应及时通知并提供证明\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"合同一方迟延履行，另一方在什么条件下可以解除合同？\",\n",
        "            \"target_articles\": [\"解除合同\", \"迟延履行\"],\n",
        "            \"key_points\": [\n",
        "                \"对方当事人在催告后在合理期限内仍未履行\",\n",
        "                \"迟延履行致使不能实现合同目的的，可以解除合同\",\n",
        "                \"解除应通知对方\"\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "    with answers_eval_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        for obj in toy_data:\n",
        "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "\n",
        "def load_answer_eval_data(path: Path) -> List[Dict[str, Any]]:\n",
        "    data = []\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "\n",
        "eval_items = load_answer_eval_data(answers_eval_path)\n",
        "print(f\"Loaded {len(eval_items)} answer-eval questions from {answers_eval_path}\")\n",
        "eval_items[:2]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 运行 RAG，生成答案\n",
        "\n",
        "- 输入：question\n",
        "- 调用：`pipeline.answer(question, top_k=...)`\n",
        "- 返回：\n",
        "  - RAG 答案文本\n",
        "  - 命中条文列表（article_no + 章节 + 预览）\n",
        "\n",
        "后续评估既会看答案文本，也会看命中条文信息。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from typing import Tuple\n",
        "\n",
        "def run_rag(question: str, top_k: int = 8) -> RagAnswer:\n",
        "    \"\"\"调用 RagPipeline answer，返回 RagAnswer 对象。\"\"\"\n",
        "    logger.info(f\"[RAG-EVAL] question={question}\")\n",
        "    ans = pipeline.answer(question, top_k=top_k)\n",
        "    return ans\n",
        "\n",
        "\n",
        "# 测试单条\n",
        "test_q = eval_items[0][\"question\"]\n",
        "print(\"Test question:\", test_q)\n",
        "test_ans = run_rag(test_q, top_k=8)\n",
        "print(\"\\n【RAG Answer Preview】\\n\")\n",
        "print(test_ans.answer[:600])\n",
        "print(\"\\n【Top hits preview】\\n\")\n",
        "for h in test_ans.hits[:3]:\n",
        "    c = h.chunk\n",
        "    print(f\"rank={h.rank}, score={h.score:.4f}, article_no={getattr(c, 'article_no', '')}\")\n",
        "    print((c.text or \"\").replace(\"\\n\", \" \")[:80] + \"...\")\n",
        "    print(\"-\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 启发式答案评估：关键点覆盖率 + 条文引用\n",
        "\n",
        "一个 **简单但可用的 heuristic 评估器**：\n",
        "作用：快速筛查“明显答非所问”或“关键点缺失严重”的情况，剩余部分交给人工或 LLM-judge。\n"
        "\n",
        "对于每个样本：\n",
        "\n",
        "1. 计算 **关键点覆盖率（coverage）**：\n",
        "   - 对 `key_points` 中每一条，检查是否在生成答案中出现（或以子串形式出现）\n",
        "   - coverage = 命中的关键点数量 / 总关键点数量\n",
        "2. 检查 **条文引用情况（article_mention_score）**：\n",
        "   - 如果答案文本中出现了 `target_articles` 中任意字符串，加分\n",
        "   - 或命中条文 `article_no` 在 `target_articles` 中出现，加分\n",
        "3. 组合成一个总分 `final_score`：\n",
        "   - `final_score = 0.7 * coverage + 0.3 * article_mention_score`\n",
        "\n"
        
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def compute_coverage(answer_text: str, key_points: List[str]) -> float:\n",
        "    if not key_points:\n",
        "        return 0.0\n",
        "    ans = answer_text or \"\"\n",
        "    matched = 0\n",
        "    for kp in key_points:\n",
        "        kp = (kp or \"\").strip()\n",
        "        if not kp:\n",
        "            continue\n",
        "        if kp in ans:\n",
        "            matched += 1\n",
        "    if matched == 0:\n",
        "        return 0.0\n",
        "    return matched / len(key_points)\n",
        "\n",
        "\n",
        "def compute_article_mention_score(answer_text: str, hits: List[Any], target_articles: List[str]) -> float:\n",
        "    if not target_articles:\n",
        "        return 0.0\n",
        "\n",        
        "    ans = answer_text or \"\"\n",
        "    score = 0.0\n",
        "    # 1) 答案文本中直接提到\n",
        "    for t in target_articles:\n",
        "        t = (t or \"\").strip()\n",
        "        if not t:\n",
        "            continue\n",
        "        if t in ans:\n",
        "            score += 0.5\n",
        "            break\n",
        "\n",
        "    # 2) 命中条文的 article_no / 文本中涉及 target_articles\n",
        "    for h in hits:\n",
        "        c = h.chunk\n",
        "        article_no = getattr(c, \"article_no\", \"\") or \"\"\n",
        "        text = (c.text or \"\")\n",
        "        for t in target_articles:\n",
        "            t = (t or \"\").strip()\n",
        "            if not t:\n",
        "                continue\n",
        "            if t in article_no or t in text:\n",
        "                score += 0.5\n",
        "                # 不重复加太多，做个上限\n",
        "                break\n",
        "        if score >= 1.0:\n",
        "            break\n",
        "\n",
        "    return min(score, 1.0)\n",
        "\n",
        "\n",
        "def evaluate_answer_item(item: Dict[str, Any], rag_answer: RagAnswer) -> Dict[str, Any]:\n",
        "    q = item.get(\"question\", \"\")\n",
        "    key_points = item.get(\"key_points\", [])\n",
        "    target_articles = item.get(\"target_articles\", [])\n",
        "\n",
        "    ans_text = rag_answer.answer or \"\"\n",
        "    hits = rag_answer.hits or []\n",
        "\n",
        "    coverage = compute_coverage(ans_text, key_points)\n",
        "    article_score = compute_article_mention_score(ans_text, hits, target_articles)\n",
        "    final_score = 0.7 * coverage + 0.3 * article_score\n",
        "\n",
        "    return {\n",
        "        \"question\": q,\n",
        "        \"answer\": ans_text,\n",
        "        \"coverage\": coverage,\n",
        "        \"article_score\": article_score,\n",
        "        \"final_score\": final_score,\n",
        "        \"target_articles\": target_articles,\n",
        "        \"key_points\": key_points,\n",
        "        \"hits\": [\n",
        "            {\n",
        "                \"rank\": h.rank,\n",
        "                \"score\": float(h.score),\n",
        "                \"article_no\": getattr(h.chunk, \"article_no\", \"\"),\n",
        "                \"chapter\": getattr(h.chunk, \"chapter\", \"\"),\n",
        "                \"section\": getattr(h.chunk, \"section\", \"\"),\n",
        "                \"text_preview\": (h.chunk.text or \"\").replace(\"\\n\", \" \")[:120] + \"...\",\n",
        "            }\n",
        "            for h in hits\n",
        "        ],\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 对整套评估数据批量运行\n",
        "\n",
        "这一节：\n",
        "\n",
        "- 对 `contract_law_qa_answers.jsonl` 中的所有问题：\n",
        "  - 调用 `run_rag` 生成答案\n",
        "  - 调用 `evaluate_answer_item` 做 heuristic 打分\n",
        "- 汇总成 DataFrame，方便排序 / 过滤\n",
        "- 同时导出一份 JSONL，后续可以人工标注或再做 LLM-judge\n",
        "\n",
        "> 注意：这一段会对每条样本发起一次 RAG 调用，若使用云端 LLM 会产生 API 费用；请先在 toy 集合上验证流程，再扩展真实数据。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results: List[Dict[str, Any]] = []\n",
        "\n",
        "for idx, item in enumerate(eval_items, start=1):\n",
        "    q = item.get(\"question\", \"\")\n",
        "    if not q:\n",
        "        continue\n",
        "    print(f\"===== [{idx}/{len(eval_items)}] {q}\")\n",
        "    try:\n",
        "        rag_ans = run_rag(q, top_k=8)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"RAG failed on question: {q}; error={e}\")\n",
        "        continue\n",
        "\n",
        "    eval_row = evaluate_answer_item(item, rag_ans)\n",
        "    results.append(eval_row)\n",
        "    print(\n",
        "        f\"  -> coverage={eval_row['coverage']:.3f}, article_score={eval_row['article_score']:.3f}, final={eval_row['final_score']:.3f}\"\n",
        "    )\n",
        "\n",
        "print(f\"\\nFinished RAG eval on {len(results)} items.\")\n",
        "\n",
        "df_eval = pd.DataFrame([\n",
        "    {\n",
        "        \"question\": r[\"question\"],\n",
        "        \"coverage\": r[\"coverage\"],\n",
        "        \"article_score\": r[\"article_score\"],\n",
        "        \"final_score\": r[\"final_score\"],\n",
        "    }\n",
        "    for r in results\n",
        "])\n",
        "\n",
        "df_eval.sort_values(\"final_score\", ascending=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 汇总统计 + 可视化\n",
        "\n",
        "统计：\n",
        "\n",
        "- 平均 coverage\n",
        "- 平均 article_score\n",
        "- 平均 final_score\n",
        "- final_score 分布（直方图）\n",
        "\n",
        "可以根据分布设置一个阈值（例如 0.6），检查：\n",
        "\n",
        "- 有多少比例的问题得分 >= 阈值\n",
        "- 对低于阈值的样本，人工审查生成答案并改进 prompt / 路由 / law_graph / 检索策略。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if not df_eval.empty:\n",
        "    print(\"Average coverage:\", df_eval[\"coverage\"].mean())\n",
        "    print(\"Average article_score:\", df_eval[\"article_score\"].mean())\n",
        "    print(\"Average final_score:\", df_eval[\"final_score\"].mean())\n",
        "\n",
        "    threshold = 0.6\n",
        "    ratio = (df_eval[\"final_score\"] >= threshold).mean()\n",
        "    print(f\"\\nFraction of questions with final_score >= {threshold}: {ratio:.3f}\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "    ax.hist(df_eval[\"final_score\"].values, bins=10)\n",
        "    ax.set_xlabel(\"final_score\")\n",
        "    ax.set_ylabel(\"count\")\n",
        "    ax.set_title(\"Distribution of RAG final_score (heuristic)\")\n",
        "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"df_eval is empty, nothing to summarize.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 导出详细结果 JSONL（便于人工审核 / LLM-judge）\n",
        "\n",
        "把 `results` 导出为 JSONL：\n",
        "\n",
        "- 路径：`data/eval/contract_law_rag_eval_results.jsonl`\n",
        "- 每一行包含：\n",
        "  - question\n",
        "  - answer（完整文本）\n",
        "  - key_points / target_articles\n",
        "  - coverage / article_score / final_score\n",
        "  - 检索命中条文列表\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rag_eval_out = EVAL_DIR / \"contract_law_rag_eval_results.jsonl\"\n",
        "\n",
        "with rag_eval_out.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for r in results:\n",
        "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Exported RAG eval results to:\", rag_eval_out)\n",
        "\n",
        "# 查看前两条\n",
        "with rag_eval_out.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        print(line.strip()[:200] + \"...\")\n",
        "        if i >= 1:\n",
        "            break"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. LLM-judge：使用独立法律模型做答案裁判\\n",
        "\\n",
        "在前面的启发式评估基础上，增加一层 **LLM-judge 评估**：\\n",
        "\\n",
        "- 使用一个单独的 LLM（例如 OpenAI gpt-4.1-mini）作为“法律裁判”\\n",
        "- 输入：问题、RAG 生成答案、预期关键点、命中条文预览\\n",
        "- 输出：\\n",
        "  - correctness：法律正确性（0–5）\\n",
        "  - completeness：完整性（0–5）\\n",
        "  - risk：误导/风险程度（0–5，越高越危险）\\n",
        "  - overall：总体评分（0.0–1.0）\\n",
        "  - comments：简短中文点评\\n",
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === LLM-judge 配置（可选） ===================\\n",
        "\\n",
        "import os\\n",
        "import time\\n",
        "from typing import Optional\\n",
        "\\n",
        "from openai import OpenAI\\n",
        "\\n",
        "USE_LLM_JUDGE = True        # 如暂时不想跑，可改为 False\\n",
        "JUDGE_MODEL = \"gpt-4.1-mini\"  # 可以换模型名字\\n",
        "\\n",
        "if USE_LLM_JUDGE:\\n",
        "    api_key = os.getenv(\"OPENAI_API_KEY\")\\n",
        "    if not api_key:\\n",
        "        print(\"[WARN] OPENAI_API_KEY not set, LLM-judge will fail to call API.\")\\n",
        "    judge_client = OpenAI()\\n",
        "    print(f\"[LLM-JUDGE] Using model: {JUDGE_MODEL}\")\\n",
        "else:\\n",
        "    judge_client = None\\n",
        "    print(\"[LLM-JUDGE] Disabled (USE_LLM_JUDGE = False)\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. 定义 LLM-judge 的打分逻辑\\n",
        "\\n",
        "这一节：\\n",
        "\\n",
        "- 构造给 LLM-judge 的 JSON 形式 prompt（包含：question / key_points / target_articles / answer / hits 预览）\\n",
        "- 要求 LLM 严格返回一个带有 5 个字段的 JSON：\\n",
        "  - correctness, completeness, risk, overall, comments\\n",
        "- 解析 JSON，得到结构化打分结果。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === 定义 LLM-judge 打分函数 ========================================\\n",
        "\\n",
        "JUDGE_SYSTEM_PROMPT = \"\"\"\\n",
        "You are an experienced Chinese contract law expert.\\n",
        "You will receive:\\n",
        "- a user question about Chinese contract law,\\n",
        "- a model-generated answer,\\n",
        "- optional key points that the answer is expected to cover,\\n",
        "- optional retrieved law articles (excerpts).\\n",
        "\\n",
        "Your task is to evaluate the QUALITY of the answer ONLY, along dimensions:\\n",
        "1) legal_correctness: whether the legal conclusion and reasoning are correct according to general principles of the PRC Civil Code (Contract Part).\\n",
        "2) completeness: whether the answer covers the key points that a competent lawyer would reasonably mention.\\n",
        "3) risk_level: how risky or misleading the answer is if used by a non-lawyer in practice.\\n",
        "\\n",
        "Return STRICTLY a JSON object with fields:\\n",
        "- correctness: integer from 0 to 5 (5 = fully correct, 0 = clearly wrong)\\n",
        "- completeness: integer from 0 to 5 (5 = very complete, 0 = almost nothing useful)\\n",
        "- risk: integer from 0 to 5 (5 = very risky / misleading, 0 = very safe)\\n",
        "- overall: float from 0.0 to 1.0 summarizing your judgement\\n",
        "- comments: short string in Chinese explaining your judgement\\n",
        "\\n",
        "Do NOT include any extra text outside the JSON.\\n",
        "\"\"\"\\n",
        "\\n",
        "def build_judge_prompt(item: dict, rag_row: dict) -> str:\\n",
        "    \"\"\"\\n",
        "    item: 原始 eval item，包含 question / key_points / target_articles\\n",
        "    rag_row: 对应的 eval_row（evaluate_answer_item 的结果）\\n",
        "    \"\"\"\\n",
        "    q = item.get(\"question\", \"\")\\n",
        "    key_points = item.get(\"key_points\", [])\\n",
        "    target_articles = item.get(\"target_articles\", [])\\n",
        "    answer = rag_row.get(\"answer\", \"\")\\n",
        "    hits = rag_row.get(\"hits\", [])\\n",
        "\\n",
        "    payload = {\\n",
        "        \"question\": q,\\n",
        "        \"expected_key_points\": key_points,\\n",
        "        \"target_articles_hint\": target_articles,\\n",
        "        \"model_answer\": answer,\\n",
        "        \"retrieved_hits_preview\": hits[:5],  # 避免太长，最多带前 5 条命中\\n",
        "    }\\n",
        "\\n",
        "    return json.dumps(payload, ensure_ascii=False, indent=2)\\n",
        "\\n",
        "\\n",
        "def call_llm_judge(payload: str, retries: int = 3, delay: float = 1.0) -> Optional[dict]:\\n",
        "    \"\"\"\\n",
        "    调用 LLM-judge，返回解析后的 dict：\\n",
        "    {\\n",
        "      \"correctness\": int,\\n",
        "      \"completeness\": int,\\n",
        "      \"risk\": int,\\n",
        "      \"overall\": float,\\n",
        "      \"comments\": str\\n",
        "    }\\n",
        "    \"\"\"\\n",
        "    if not USE_LLM_JUDGE or judge_client is None:\\n",
        "        return None\\n",
        "\\n",
        "    last_err = None\\n",
        "    for _ in range(retries):\\n",
        "        try:\\n",
        "            resp = judge_client.chat.completions.create(\\n",
        "                model=JUDGE_MODEL,\\n",
        "                messages=[\\n",
        "                    {\"role\": \"system\", \"content\": JUDGE_SYSTEM_PROMPT},\\n",
        "                    {\\n",
        "                        \"role\": \"user\",\\n",
        "                        \"content\": (\\n",
        "                            \"下面是一个法律问答样本，请根据说明打分，并严格返回 JSON：\\n\\n\"\\n",
        "                            + payload\\n",
        "                        ),\\n",
        "                    },\\n",
        "                ],\\n",
        "                temperature=0.0,\\n",
        "            )\\n",
        "            raw = resp.choices[0].message.content.strip()\\n",
        "            # 有些模型可能在前后加标记，这里做个简单清洗\\n",
        "            raw = raw.strip(\"` \\n\")\\n",
        "            data = json.loads(raw)\\n",
        "            return data\\n",
        "        except Exception as e:\\n",
        "            last_err = e\\n",
        "            time.sleep(delay)\\n",
        "\\n",
        "    print(\"[LLM-JUDGE] Failed to get valid JSON after retries, last error:\", last_err)\\n",
        "    return None"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. 批量运行 LLM-judge，对每条样本打分\\n",
        "\\n",
        "这一节会：\\n",
        "\\n",
        "- 遍历前若干条 `results`（默认最多 20 条）\\n",
        "- 为每条构造 LLM-judge 的输入 payload\\n",
        "- 调用 LLM，获取 correctness / completeness / risk / overall / comments\\n",
        "- 与启发式分数（coverage / article_score / final_score）一起保存为 `df_judge`。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === 批量运行 LLM-judge，并与 heuristic 分数对比 ====================\\n",
        "\\n",
        "if not USE_LLM_JUDGE:\\n",
        "    print(\"[LLM-JUDGE] Skipped (set USE_LLM_JUDGE = True to enable).\")\\n",
        "else:\\n",
        "    judge_rows = []\\n",
        "\\n",
        "    # 为防止数据集过大，先限制最大样本数\\n",
        "    MAX_JUDGE_SAMPLES = 20  # 可根据需要调整\\n",
        "\\n",
        "    n = min(len(results), MAX_JUDGE_SAMPLES)\\n",
        "    print(f\"[LLM-JUDGE] Evaluating {n} samples with model {JUDGE_MODEL}...\")\\n",
        "\\n",
        "    for idx in range(n):\\n",
        "        item = eval_items[idx]\\n",
        "        rag_row = results[idx]\\n",
        "\\n",
        "        payload = build_judge_prompt(item, rag_row)\\n",
        "        judge_res = call_llm_judge(payload)\\n",
        "\\n",
        "        if judge_res is None:\\n",
        "            print(f\"[LLM-JUDGE] Sample {idx+1}/{n} -> judge failed.\")\\n",
        "            continue\\n",
        "\\n",
        "        row = {\\n",
        "            \"question\": rag_row[\"question\"],\\n",
        "            \"heuristic_final\": rag_row[\"final_score\"],\\n",
        "            \"heuristic_coverage\": rag_row[\"coverage\"],\\n",
        "            \"heuristic_article_score\": rag_row[\"article_score\"],\\n",
        "            \"judge_correctness\": judge_res.get(\"correctness\"),\\n",
        "            \"judge_completeness\": judge_res.get(\"completeness\"),\\n",
        "            \"judge_risk\": judge_res.get(\"risk\"),\\n",
        "            \"judge_overall\": judge_res.get(\"overall\"),\\n",
        "            \"judge_comments\": judge_res.get(\"comments\", \"\"),\\n",
        "        }\\n",
        "        judge_rows.append(row)\\n",
        "\\n",
        "        print(\\n",
        "            f\"[{idx+1}/{n}] final={row['heuristic_final']:.3f} | \"\\n",
        "            f\"judge_overall={row['judge_overall']} \"\\n",
        "            f\"(correctness={row['judge_correctness']}, completeness={row['judge_completeness']}, risk={row['judge_risk']})\"\\n",
        "        )\\n",
        "\\n",
        "    if not judge_rows:\\n",
        "        print(\"[LLM-JUDGE] No valid judged samples.\")\\n",
        "    else:\\n",
        "        df_judge = pd.DataFrame(judge_rows)\\n",
        "        display(df_judge.head())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. 分析 heuristic 分数与 LLM-judge 分数的相关性\\n",
        "\\n",
        "- 计算 `heuristic_final` 与 `judge_overall` 的相关系数\\n",
        "- 画散点图观察两者关系\\n",
        "- 列出：\\n",
        "  - 启发式高但 LLM-judge 低的样本（可能 heuristic 误判）\\n",
        "  - 启发式低但 LLM-judge 高的样本（可能 heuristic 太苛刻）\\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === 分析 heuristic 分数与 LLM-judge 分数的相关性 =====================\\n",
        "\\n",
        "if USE_LLM_JUDGE and \"df_judge\" in globals() and not df_judge.empty:\\n",
        "    # 过滤掉 judge_overall 为空或非数值的情况\\n",
        "    df_valid = df_judge.dropna(subset=[\"judge_overall\"]).copy()\\n",
        "\\n",
        "    # 尝试把 judge_overall 转成 float\\n",
        "    df_valid[\"judge_overall\"] = df_valid[\"judge_overall\"].astype(float)\\n",
        "\\n",
        "    if not df_valid.empty:\\n",
        "        corr = np.corrcoef(df_valid[\"heuristic_final\"], df_valid[\"judge_overall\"])[0, 1]\\n",
        "        print(f\"Pearson correlation between heuristic_final and judge_overall: {corr:.3f}\")\\n",
        "\\n",
        "        # 散点图：x = heuristic_final, y = judge_overall\\n",
        "        fig, ax = plt.subplots(figsize=(5, 4))\\n",
        "        ax.scatter(df_valid[\"heuristic_final\"], df_valid[\"judge_overall\"])\\n",
        "        ax.set_xlabel(\"Heuristic final_score\")\\n",
        "        ax.set_ylabel(\"LLM-judge overall\")\\n",
        "        ax.set_title(\"Heuristic vs LLM-judge scores\")\\n",
        "        ax.grid(True, linestyle=\"--\", alpha=0.3)\\n",
        "        plt.tight_layout()\\n",
        "        plt.show()\\n",
        "\\n",
        "        print(\"\\n[Cases] Heuristic high but judge low:\")\\n",
        "        display(\\n",
        "            df_valid.sort_values([\"heuristic_final\", \"judge_overall\"], ascending=[False, True])\\n",
        "                   .head(3)[[\"question\", \"heuristic_final\", \"judge_overall\", \"judge_comments\"]]\\n",
        "        )\\n",
        "\\n",
        "        print(\"\\n[Cases] Heuristic low but judge high:\")\\n",
        "        display(\\n",
        "            df_valid.sort_values([\"heuristic_final\", \"judge_overall\"], ascending=[True, False])\\n",
        "                   .head(3)[[\"question\", \"heuristic_final\", \"judge_overall\", \"judge_comments\"]]\\n",
        "        )\\n",
        "    else:\\n",
        "        print(\"[LLM-JUDGE] No valid rows after cleaning judge_overall.\")\\n",
        "else:\\n",
        "    print(\"[LLM-JUDGE] df_judge not available or judge disabled.\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
