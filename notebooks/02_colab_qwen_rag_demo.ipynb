{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Legal-RAG: Colab Demo\n",
        "\n",
        "1. 克隆 Legal-RAG 仓库\n",
        "2. 安装依赖 \n",
        "3. 设置LLM， 如 Qwen2-1.5B-Instruct\n",
        "4. 构建索引\n",
        "5. 通过 `RagPipeline` 对真实法律问题进行问答\n",
        "\n",
        "---\n",
        "在 Colab 中：\n",
        "- `Runtime` → `Change runtime type` → `Hardware accelerator` 选择 `GPU`\n",
        "- 如果使用 Hugging Face Hub 下载 Qwen 模型，可以在环境变量中配置 `HF_TOKEN`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!nvidia-smi || echo \"No GPU detected.\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 克隆仓库 & 安装依赖\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path(\"/content\").resolve()\n",
        "os.chdir(ROOT)\n",
        "print(\"ROOT:\", ROOT)\n",
        "\n",
        "REPO_URL = \"https://github.com/Fan-Luo/Legal-RAG.git\"  # 仓库地址\n",
        "REPO_DIR = ROOT / \"Legal-RAG\"\n",
        "os.environ[\"REPO_DIR\"] = str(REPO_DIR)\n",
        "\n",
        "if not REPO_DIR.exists():\n",
        "    !git clone \"$REPO_URL\" \"$REPO_DIR\"\n",
        "else:\n",
        "    %cd \"$REPO_DIR\"\n",
        "    !git pull\n",
        "\n",
        "%cd \"$REPO_DIR\"\n",
        "print(\"Current dir:\", Path.cwd())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%bash\n",
        "set -e\n",
        "cd \"$REPO_DIR\" || exit 1\n",
        "\n",
        "pip install -q -r requirements.txt\n",
        "# 保险起见补充：transformers / accelerate / bitsandbytes\n",
        "pip install -q transformers accelerate bitsandbytes\n",
        "\n",
        "echo \"Dependencies installed.\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 准备数据 & 构建索引（如已构建可跳过）\n",
        "\n",
        "需要：\n",
        "- `data/raw/minfadian_hetongbian.txt`（可以手动上传后复制到该路径）\n",
        "- 然后运行预处理脚本和建索引脚本。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "%cd \"$REPO_DIR\"\n",
        "\n",
        "RAW_DIR = Path(\"data/raw\")\n",
        "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"Raw dir:\", RAW_DIR.resolve())\n",
        "print(\"Raw files:\", list(RAW_DIR.iterdir()))\n",
        "\n",
        "# 如有需要，运行预处理与建索引脚本\n",
        "try:\n",
        "    !python -m scripts.preprocess_law\n",
        "except SystemExit:\n",
        "    print(\"Preprocessing completed.\")\n",
        "\n",
        "try:\n",
        "    !python -m scripts.build_index\n",
        "except SystemExit:\n",
        "    print(\"Index building completed.\")\n",
        "\n",
        "print(\"Processed:\")\n",
        "!ls -R data/processed || echo \"no processed dir\"\n",
        "print(\"Index:\")\n",
        "!ls -R data/index || echo \"no index dir\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 配置 LLM（在 Notebook 里切换模型）\n",
        "\n",
        "这里通过 `AppConfig.load()` 加载默认配置，然后在 Notebook 中修改：\n",
        "\n",
        "- `cfg.llm.provider = \"qwen-local\"`\n",
        "- `cfg.llm.model = \"Qwen/Qwen2-1.5B-Instruct\"`\n",
        "\n",
        "也可以改成 OpenAI 的模型：\n",
        "\n",
        "```python\n",
        "cfg.llm.provider = \"openai\"\n",
        "cfg.llm.model = \"gpt-4.1-mini\"  # 需要在环境变量设置 OPENAI_API_KEY\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from legalrag.config import AppConfig\n",
        "from legalrag.pipeline.rag_pipeline import RagPipeline\n",
        "",
        "# 使用 AppConfig.load()，自动补全各类路径并创建目录\n",
        "cfg = AppConfig.load()\n",
        "",
        "# 在 Notebook 里配置要使用的模型\n",
        "cfg.llm.provider = \"qwen-local\"  # 或 \"openai\"\n",
        "cfg.llm.model = \"Qwen/Qwen2-1.5B-Instruct\"  # 可改成其他 Qwen 模型或本地路径\n",
        "",
        "print(\"LLM provider:\", cfg.llm.provider)\n",
        "print(\"LLM model   :\", cfg.llm.model)\n",
        "",
        "pipeline = RagPipeline(cfg)\n",
        "",
        "def ask(question: str, top_k: int = 8):\n",
        "    print(\"问题：\", question)\n",
        "    print(\"=\" * 80)\n",
        "    ans = pipeline.answer(question, top_k=top_k)\n",
        "    print(\"【回答】\\n\")\n",
        "    print(ans.answer)\n",
        "    print(\"\\n【命中条文】\\n\")\n",
        "    for h in ans.hits:\n",
        "        c = h.chunk\n",
        "        print(f\"[{h.rank}] 第{c.article_no}条 ({c.chapter} / {c.section}) score={h.score:.4f}\")\n",
        "        preview = (c.text or \"\")[:80].replace(\"\\n\", \" \")\n",
        "        print(\"   \", preview, \"...\")\n",
        "        print()",
        "",
        "q1 = \"合同约定的违约金为合同金额的 40%，是否合理？\"\n",
        "ask(q1, top_k=8)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
